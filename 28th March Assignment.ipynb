{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb031886-ac81-4753-a7db-de7e0f16f170",
   "metadata": {},
   "source": [
    "Question 1 : What is Ridge Regression, and how does it differ from ordinary least squares regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ef98c3d-e257-4670-836c-1a4d673c039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer_one = \"\"\" \n",
    "Ridge Regression is a linear regression technique used for dealing with multicollinearity, which occurs when predictor variables in a multiple\n",
    "regression model are highly correlated with each other.\n",
    "In Ridge Regression, a penalty term is added to the sum of squared errors, which helps to reduce the magnitude of the coefficients of\n",
    "highly correlated variables.\n",
    "\n",
    "The penalty term in Ridge Regression is called the L2 regularization, which is represented by the square of the L2 norm of the coefficients.\n",
    "By adding the L2 regularization term to the ordinary least squares (OLS) objective function,\n",
    "Ridge Regression shrinks the coefficients of highly correlated variables towards zero, without completely eliminating them,\n",
    "which results in a better generalization performance of the model on unseen data.\n",
    "\n",
    "In contrast, Ordinary Least Squares Regression (OLS) is a simple linear regression technique that aims to minimize the sum of squared errors between\n",
    "the predicted and actual values. OLS does not include any regularization term and assumes that the predictor variables are independent of each other.\n",
    "As a result, OLS may not perform well when multicollinearity is present, as it can lead to unstable and overfit models.\n",
    "\n",
    "In summary, Ridge Regression is a regularized version of linear regression that helps to reduce multicollinearity and improves\n",
    "the performance of the model on unseen data, while OLS is a simple linear regression technique that does not account for multicollinearity and\n",
    "may not generalize well when it is present.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bd2fad-fbf5-4827-a070-f73c14725fc2",
   "metadata": {},
   "source": [
    "Question 2 : What are the assumptions of Ridge Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d84cbc34-4041-4e6f-ac85-3e2fd07ace16",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer_two = \"\"\" \n",
    "Ridge Regression is a linear regression technique that is based on the same underlying assumptions as Ordinary Least Squares (OLS) regression.\n",
    "In addition to these assumptions, Ridge Regression also assumes that:\n",
    "\n",
    "1. There is no perfect multicollinearity: Ridge Regression assumes that there is no perfect linear relationship between any of the predictor variables,\n",
    "in the model. If there is perfect multicollinearity, the matrix of predictor variables will be singular and the regression coefficients,\n",
    "will not be unique.\n",
    "\n",
    "2. The errors are normally distributed: Ridge Regression assumes that the errors are normally distributed with mean zero and constant variance.\n",
    "If the errors are not normally distributed, the model may be biased and inefficient.\n",
    "\n",
    "3. The errors are independent: Ridge Regression assumes that the errors are independent of each other. If the errors are correlated,\n",
    "the model may be inefficient and the standard errors of the coefficients may be underestimated.\n",
    "\n",
    "4. The relationship between the predictors and response is linear: Ridge Regression assumes that the relationship between the predictor variables,\n",
    "and the response variable is linear. If the relationship is non-linear, Ridge Regression may not provide a good fit to the data.\n",
    "\n",
    "5. The model is correctly specified: Ridge Regression assumes that the model is correctly specified and includes all relevant predictor variables.\n",
    "If the model is misspecified, the estimates of the coefficients may be biased.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671f818f-8a13-47d1-8cf9-be25b493a07a",
   "metadata": {},
   "source": [
    "Question 3 : How do you select the value of the tuning parameter (lambda) in Ridge Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12473e6c-7922-4b85-a91e-7c01f10e50a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer_three = \"\"\" \n",
    "The value of the tuning parameter, lambda, in Ridge Regression determines the strength of the regularization and has a significant impact\n",
    "on the performance of the model.\n",
    "Selecting an appropriate value of lambda is important to balance the trade-off between bias and variance.\n",
    "\n",
    "One common approach for selecting the value of lambda in Ridge Regression is to use cross-validation.\n",
    "In k-fold cross-validation, the dataset is divided into k subsets of equal size, and each subset is used as a validation set\n",
    "while the remaining k-1 subsets are used as the training set. The model is trained on the training set for each fold,\n",
    "and the validation error is calculated for each value of lambda.\n",
    "The optimal value of lambda is the one that minimizes the average validation error across all the folds.\n",
    " \n",
    "Another approach is to use the generalized cross-validation (GCV) method, which is a computationally efficient method for selecting the optimal\n",
    "value of lambda.\n",
    "The GCV method involves calculating the GCV score for each value of lambda, which is a measure of the model's fit to the data.\n",
    "The optimal value of lambda is the one that minimizes the GCV score.\n",
    " \n",
    "There are also other methods for selecting the value of lambda in Ridge Regression, such as the Bayesian information criterion (BIC)\n",
    "and the Akaike information criterion (AIC), which are based on information theory principles.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f6fe9d-77d4-471d-ba19-235be6324c82",
   "metadata": {},
   "source": [
    "Question 4 : Can Ridge Regression be used for feature selection? If yes, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75fe93b1-0608-438e-96fc-d5bd2634e4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer_four = \"\"\" \n",
    "Yes, Ridge Regression can be used for feature selection by shrinking the coefficients of less important features towards zero.\n",
    "The shrinkage effect of Ridge Regression is proportional to the magnitude of the coefficients,\n",
    "so features with smaller coefficients will be more heavily penalized and tend to be reduced to zero.\n",
    "This results in a subset of the most important features being selected and the less important features being excluded from the model.\n",
    "\n",
    "To use Ridge Regression for feature selection, the first step is to standardize the predictor variables to have zero mean and unit variance.\n",
    "This is important because Ridge Regression is sensitive to the scale of the predictor variables,\n",
    "and standardizing them ensures that the regularization penalty is applied equally to all variables.\n",
    "\n",
    "The second step is to fit a Ridge Regression model with a range of values for the tuning parameter, lambda\n",
    ", using cross-validation or another method for selecting the optimal value of lambda.\n",
    "The Ridge Regression coefficients are then obtained for each value of lambda\n",
    ", and the magnitude of the coefficients can be used to rank the importance of the predictor variables.\n",
    "\n",
    "Finally, a subset of the most important variables can be selected by choosing a threshold for the coefficient magnitudes,\n",
    "or by using a feature selection algorithm that takes into account the coefficients obtained from Ridge Regression.\n",
    "\n",
    "It is important to note that Ridge Regression may not always select the optimal subset of predictor variables,\n",
    "as it tends to shrink all coefficients towards zero to some extent. Other feature selection methods,\n",
    "such as Lasso Regression or Elastic Net Regression, may be more suitable for selecting sparse subsets of predictor variables that\n",
    "are most relevant for predicting the response variable.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef151f1d-dcf0-4d4a-970d-2ce678cb5110",
   "metadata": {},
   "source": [
    "Question 5 : How does the Ridge Regression model perform in the presence of multicollinearity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1e0ffe-df97-4cc4-b06e-1105011b791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer_five = \"\"\"\n",
    "Ridge Regression can perform well in the presence of multicollinearity, which occurs when there is a high degree of correlation,\n",
    "among the predictor variables in a regression model.\n",
    "Multicollinearity can lead to unstable estimates of the regression coefficients and can make it difficult to interpret\n",
    "the effects of individual predictor variables on the response variable.\n",
    "\n",
    "In Ridge Regression, the penalty term added to the sum of squared residuals is proportional to the square of the L2 norm of the regression\n",
    "coefficients, which shrinks the coefficients towards zero and reduces the impact of multicollinearity on the estimation of the coefficients.\n",
    "This means that Ridge Regression can help to stabilize the estimates of the regression coefficients and reduce the variance of the estimates.\n",
    "\n",
    "However, Ridge Regression does not completely eliminate the problem of multicollinearity,\n",
    "as it only reduces the impact of multicollinearity by shrinking the coefficients towards zero.\n",
    "If the degree of multicollinearity is very high, Ridge Regression may still produce biased and unstable estimates of the coefficients.\n",
    "In such cases, it may be necessary to use other methods, such as principal component regression, partial least squares regression, or\n",
    "variance inflation factor analysis, to deal with multicollinearity.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
