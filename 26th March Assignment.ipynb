{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34288f6c-87e2-4986-9426-8fbf9245e5cf",
   "metadata": {},
   "source": [
    "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
    "example of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4334d9-c421-4675-a2af-2cf46f6de01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer_one = \"\"\" \n",
    "Simple Linear Regression:\n",
    "1. Regression takes features as input and gives out continous numerical output or numerical prediction.\n",
    "There are many regression techniques that are used based on the pattern of independent features.\n",
    "Simple linear regression is one such technique used to predict dependent or target variable using independent variables as inputs.\n",
    "2. Equation that is made use for performing linear regression looks something like below:\n",
    "   y(prediction) = mX1 + c\n",
    "   We have different notation for m and c.\n",
    "   Here y is the prediction based on feature X1.\n",
    "   m is the slope depicting the change in prediction per unit change of feature X1.\n",
    "   c is the intercept or the initial bias we assume when not considering the feature.\n",
    "3. The above line equation is helpful in finding out the best fit line that is inturn helpful in finding out the accuracy of our model,\n",
    "by measuring mean square error.\n",
    "4. The datapoints are assumed to have followed a linear relationship.\n",
    "5. There is no visible pattern in the plot of datapoints accept being linearly related.\n",
    "6. The error terms must have constant variance. This phenomena is known as Homoscedasticity.\n",
    "\n",
    "Multiple Linear Regression.\n",
    "1. Multiple linear regression is a technique to understand the relationship between a single dependent variable and multiple independent variables.\n",
    "The equation followed in multiple regression goes like below:\n",
    "2. \n",
    "y(predicted) = c + m1x1 + m2x2 + m3x3 + .... + mpxp.\n",
    "where y is the predicted output based on feature x1,x2,x3,...,xp.\n",
    "and m1,m2,m3 represent coefficients or weights corresponding to the features x1,x2,x3,...,xp.\n",
    "c is the bias.\n",
    "\n",
    "3. Multiple linear regression do follow linear regression assumptions, but in addition to that it follows below written assumptions as well.\n",
    "a. Overfitting: When more and more variables are added to a model, the model may become far too complex and usually ends up memorizing\n",
    "all the data points in the training set. This phenomena is overfitting.\n",
    "b. Multicollinearity: It is the phenomenon where a model with several independent variables, may have some variables interrelated.\n",
    "c. Feature Selection: With more variables present. Selecting the optimal set of predictions from the pool of given features\n",
    "becomes an important task for building a relevant and better model.\n",
    "\n",
    "\n",
    "Example of simple Linear regression\n",
    "Example of Simple Linear Regression: Suppose we want to determine whether there is a relationship between the amount of time (in minutes),\n",
    "a person spends exercising each day (independent variable) and their resting heart rate (dependent variable). We collect data on 50 individuals,\n",
    "including their daily exercise time and their resting heart rate, and plot the data points on a scatter plot.\n",
    "We can then fit a straight line to the data points that best represents the relationship between exercise time and resting heart rate. \n",
    "We can use this line to predict the resting heart rate of individuals based on the amount of time they spend exercising each day.\n",
    "\n",
    "Example of multiple linear regression.\n",
    "Suppose we have a dataset that includes the salaries (dependent variable) of 100 employees,\n",
    "along with their age, education level, and years of experience (independent variables).\n",
    "We can use multiple linear regression to determine how well age, education level, and years of experience predict salary.\n",
    "We can fit a linear equation that includes all three independent variables to the data points,\n",
    "and use this equation to make predictions about the salary of new employees based on their age, education level, and years of experience.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f0e5aa-1f01-44d1-b31a-8640d0a3d9e2",
   "metadata": {},
   "source": [
    "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
    "a given dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f15a383-aec4-4e48-a44e-94b6d229f344",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer_two = \"\"\" \n",
    "1. Linearity: The relationship between the dependent variable and the independent variable(s) should be linear.\n",
    "This means that the change in the dependent variable should be proportional to the change in the independent variable(s).\n",
    "\n",
    "2. Independence: The observations should be independent of each other, meaning that the value of the dependent variable for one observation\n",
    "should not be related to the value of the dependent variable for any other observation.\n",
    "\n",
    "3. omoscedasticity: The variance of the errors (the difference between the observed values and the predicted values) should be\n",
    "constant across all levels of the independent variable(s). This is known as homoscedasticity.\n",
    "\n",
    "4. Normality: The errors should be normally distributed, meaning that the frequency distribution of the errors should be bell-shaped.\n",
    "\n",
    "5. No multicollinearity: In multiple linear regression, there should be no perfect linear relationship among the independent variables.\n",
    "\n",
    "Methods to check these assumptions.\n",
    "1. Scatter plot: A scatter plot can be used to visualize the relationship between the dependent variable and each independent variable.\n",
    "A linear relationship should be visible.\n",
    "\n",
    "2. Residual plot: A plot of the residuals (the difference between the observed values and the predicted values)\n",
    "against the predicted values can be used to check for homoscedasticity.\n",
    "If the variance of the residuals is constant across all levels of the independent variable(s), then the assumption of homoscedasticity holds.\n",
    "\n",
    "3. Normal probability plot: A normal probability plot of the residuals can be used to check for normality.\n",
    "If the points on the plot follow a straight line, then the assumption of normality holds.\n",
    "\n",
    "4. Variance Inflation Factor (VIF): VIF can be calculated to detect multicollinearity in multiple linear regression.\n",
    "If VIF is greater than 5, then multicollinearity is a concern.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c5d004-1d60-45dd-8422-c501c758c2f9",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
    "a real-world scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5564d2d-e7c1-48ca-9321-a9b2032cba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer_three = \"\"\" \n",
    "In a linear regression model, the slope represents the rate of change of the response variable (Y) with respect to the predictor variable (X).\n",
    "It indicates how much the response variable changes for a unit change in the predictor variable.\n",
    "The intercept represents the expected value of the response variable when the predictor variable is zero.\n",
    "For example, consider a real-world scenario where you want to predict the sales of a company based on its advertising budget.\n",
    "You collect data on the advertising budget and corresponding sales for several months and fit a linear regression model.\n",
    "\n",
    "Let's say the model is:\n",
    "Sales = 1000 + 5 * Advertising Budget\n",
    "\n",
    "In this model, the intercept of 1000 represents the expected sales when the advertising budget is zero.\n",
    "This might be due to other factors such as brand recognition, customer loyalty, or previous marketing efforts.\n",
    "The slope of 5 indicates that for every additional unit of advertising budget spent, sales are expected to increase by 5 units.\n",
    "\n",
    "So, for example, if the advertising budget is $10,000, the expected sales would be:\n",
    "Sales = 1000 + 5 * 10,000\n",
    "Sales = 51,000\n",
    "This means that the company is expected to make $51,000 in sales if they spend $10,000 on advertising, assuming other factors remain constant.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429543a6-7e30-45d7-818f-d6f59dcd688f",
   "metadata": {},
   "source": [
    "Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f24fb0d-e911-4816-98c4-eefa48727650",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer_four = \"\"\" \n",
    "1. Gradient descent is an iterative optimization algorithm used to minimize the cost function of a machine learning model.\n",
    "In simple terms, it is a way to adjust the parameters of a model to make it fit the training data better.\n",
    "\n",
    "2. The idea behind gradient descent is to calculate the gradient of the cost function with respect to each parameter and\n",
    "move in the opposite direction of the gradient to find the minimum value of the cost function.\n",
    "The gradient is a vector that indicates the direction of the steepest ascent of the cost function.\n",
    "By moving in the opposite direction, we can descend to the minimum value of the cost function.\n",
    "\n",
    "3. In machine learning, gradient descent is used to optimize the parameters of the model to minimize the error between the predicted values\n",
    "and the actual values. The cost function is a measure of the error between the predicted values and the actual values,\n",
    "and the goal is to minimize this cost function.\n",
    "\n",
    "4. There are two types of gradient descent: batch gradient descent and stochastic gradient descent. In batch gradient descent,\n",
    "the gradient is calculated using the entire training dataset.\n",
    "In stochastic gradient descent, the gradient is calculated using only one example at a time.\n",
    "\n",
    "5. Gradient descent is used in many machine learning algorithms, including linear regression, logistic regression, and neural networks.\n",
    "It is an essential part of machine learning because it allows us to find the optimal parameters of a model that best fit the training data,\n",
    "and therefore, make accurate predictions on new data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a3b603-8625-45c3-bad7-4199fbdf4166",
   "metadata": {},
   "source": [
    "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c5ce5d-5a3f-4522-bca7-cd602dedd42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer_five = \"\"\" \n",
    "Multiple linear regression is a statistical technique used to study the relationship between a dependent variable and two or\n",
    "more independent variables. It involves estimating the relationship between the dependent variable and multiple independent variables simultaneously.\n",
    "In this model, the dependent variable is predicted as a function of the independent variables.\n",
    "\n",
    "The mathematic formula for mulitple linear regression can be represented as: y(predicted) = c + m1x1 + m2x2 + m3x3 + ... + mpxp.\n",
    "where c is the bias.\n",
    "m1,m2,m3,m4,...,mp are the slopes or weights.\n",
    "X1,...,Xp are features.\n",
    "\n",
    "Multiple linear regression differs from simple linear regression in that it involves more than one independent variable.\n",
    "Simple linear regression only involves one independent variable and seeks to establish a linear relationship between the dependent variable\n",
    "and that independent variable. In contrast, multiple linear regression seeks to establish a relationship between the dependent variable and\n",
    "multiple independent variables simultaneously.\n",
    "\n",
    "The advantage of multiple linear regression over simple linear regression is that it allows for the identification of multiple factors\n",
    "that contribute to the variation in the dependent variable. This can provide a more complete understanding of the\n",
    "relationship between the dependent variable and the independent variables, and can lead to more accurate predictions.\n",
    "However, multiple linear regression can also be more complex to interpret and may require more data to provide meaningful results.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12895178-5c93-4605-a0fd-e1209b203d31",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
    "address this issue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235fc1c4-aa5c-4582-baae-f881e03a6ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer_six = \"\"\" \n",
    "Multicollinearity refers to a situation in multiple linear regression where two or more predictor variables are highly correlated with each other.\n",
    "This can cause problems in the estimation of the regression coefficients,\n",
    "as the individual effects of the collinear variables on the response variable may be difficult to distinguish.\n",
    "\n",
    "Multicollinearity can be detected by examining the correlation matrix between the predictor variables.\n",
    "A correlation coefficient close to 1 or -1 indicates a high degree of correlation between two variables,\n",
    "while a coefficient close to 0 indicates little correlation. Another way to detect multicollinearity is to look at the\n",
    "variance inflation factor (VIF), which measures the extent to which the variance of the estimated regression coefficients\n",
    "is inflated due to the multicollinearity. A VIF greater than 10 indicates a potential issue with multicollinearity.\n",
    "\n",
    "To address multicollinearity, one approach is to remove one of the correlated predictor variables from the model.\n",
    "Another approach is to use dimensionality reduction techniques, such as principal component analysis (PCA),\n",
    "to transform the correlated predictor variables into a smaller set of uncorrelated variables. Additionally,\n",
    "regularization techniques such as Ridge or Lasso regression can help mitigate the impact of multicollinearity on the regression coefficients.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153732b9-bb26-4cdb-b6a3-6cc76d4cd030",
   "metadata": {},
   "source": [
    "Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a97bef-30d8-4776-a7ce-94f7a576cb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "Anwer_seven = \"\"\" \n",
    "Polynomial regression is a type of regression analysis in which the relationship between the independent variable x and the dependent variable y\n",
    "is modeled as an nth-degree polynomial. In other words, it is a curve-fitting technique in which a polynomial function is\n",
    "used to approximate the relationship between the two variables.\n",
    "\n",
    "The basic idea behind polynomial regression is to take the basic linear regression model, which fits a straight line to the data,\n",
    "and extend it by adding polynomial terms to the equation. These polynomial terms are added by including\n",
    "higher-order powers of the independent variable x, such as x², x³, etc.\n",
    "\n",
    "Formulae fo polynomial regression goes like,\n",
    "y(predicted) = c + m1x + m2x^2 + m3x^3 + m4x^4 + ... + mpx^p.\n",
    "where y is predicted output using features x1,x2,x3,x4,...,xp.\n",
    "c is the intercept or the bias.\n",
    "m1,m2,m3,..,mp represents slopes.\n",
    "\n",
    "So, while the linear regression model assumes a linear relationship between x and y, the polynomial regression model allows for a more flexible,\n",
    "nonlinear relationship between the two variables. By fitting a polynomial function to the data,\n",
    "the model can capture more complex patterns in the data that may not be captured by a simple linear model.\n",
    "\n",
    "One of the main differences between linear regression and polynomial regression is that the former can only model linear relationships,\n",
    "while the latter can model nonlinear relationships as well. Another key difference is that the polynomial regression model can\n",
    "fit more complex patterns in the data, but it may also be more prone to overfitting if too many polynomial terms are included.\n",
    "Linear regression is often preferred when the relationship between the variables is expected to be linear,\n",
    "and when interpretability is more important than predictive power, while polynomial regression is more suitable when a nonlinear relationship\n",
    "is suspected, and predictive power is more important than interpretability.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a2ce69-6187-43fd-8716-28fd69e8fda3",
   "metadata": {},
   "source": [
    "Q8. What are the advantages and disadvantages of polynomial regression compared to linear\n",
    "regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f51911e-993d-4179-88b5-0667751e08bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer_eight = \"\"\" \n",
    "Polynomial regression is a type of regression analysis in which the relationship between the independent variable x and\n",
    "the dependent variable y is modeled as an nth degree polynomial.\n",
    "In contrast, linear regression models the relationship between the variables as a straight line.\n",
    "Here are the advantages and disadvantages of polynomial regression compared to linear regression:\n",
    "\n",
    "Advantages:\n",
    "1. Polynomial regression can fit a wide range of complex nonlinear relationships between the independent and dependent variables.\n",
    "2. It can capture the curvature in the data better than linear regression and provide a better fit to the data.\n",
    "3. It can be more accurate than linear regression when the relationship between the variables is nonlinear.\n",
    "4. Polynomial regression can provide insights into the shape and nature of the relationship between the variables.\n",
    "\n",
    "Disadvantages:\n",
    "1. Polynomial regression can easily overfit the data, especially when the degree of the polynomial is high,\n",
    "leading to poor generalization to new data.\n",
    "2. It can be computationally expensive, especially for high-degree polynomials and large datasets.\n",
    "3. Polynomial regression can be less interpretable than linear regression, as it involves fitting a more complex model.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
