{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81724ee5-296a-4523-8cee-9985a5e6eef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: What is web scrapping? Why is it used? Give three areas where web scrapping is used to get data?\n",
    "ans = \"\"\" \n",
    "    Web scrapping is an automated method that allows to obtain or scrap out large amounts of data from websites,\n",
    "    that prove out to be essential for monitoring the value of product or organization.\n",
    "    Most of this scrapped out data will be unstructured data in an HTML format which is then converted into structured data in a spreadsheet or a database.\n",
    "    \n",
    "    Why do we use web scrapping.\n",
    "    Suppose I want to know how many people liked my product, to get an estimate about it, I will search through review section,\n",
    "    check the percentage of 5* review, 4* review, 3* review, 2* review etc. Based on this gathered data I will make my decision about the product.\n",
    "    If I do this manually that is going to be a tedious and mind numbing task involving a lot of copy-pasting.\n",
    "    To avoid this manual work web scrapping concept came into picture, and is a widely used method inorder to gather data from websites.\n",
    "    \n",
    "    Areas Where web scrapping proves out to be useful.\n",
    "    1. Price Monitoring:- Web scrapping can be used by companies to scrap the product data for their products and competing products as well to see how it impacts their pricing strategies.\n",
    "    2. Market Research:- Web scrapping can be used for market research by companies. High-quality web scraped data obtained in large volumes can be very helpful for companies in analyzing consumer trends and understanding which direction the company should move in the future.\n",
    "    3. Sentiment Analysis:- If companies want to understand general sentiment for their products among their consumers, then sentiment analysis is a must. We can use web scrapping to collect data from social media websites as to know what the general sentiment about product is.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf5fd5a5-f9e1-4a32-a597-fc610cf5d035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are different methods used for web scrapping?\n",
    "ans = \"\"\" \n",
    "    1. Copy-pasting:- This is a simple but tedious approach to scrap a website, it involves manual collection of data from website.\n",
    "    2. Web scrapping software:- We can develop our own software that can help in collecting or scraping data from websites.\n",
    "    3. HTTP programming:- Using socket programming, Posting HTTP requests can help one retrieve dynamic as well as static web page information.\n",
    "    4. DOM parsing:- Client-side scripts parse the contents of web page into DOM(Document object model) tree. By embedding a program into the web browser, we can retrieve the information from the tree.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "602bb6a3-9d81-4526-a694-28d974551e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is beautiful soup? Why is it used?\n",
    "ans = \"\"\"  \n",
    "    Beautiful soup is a python library designed for projects like web-scrapping, image-scrapping.\n",
    "    If provides a few simple methods for searching, navigating and modifying parse tree. A toolkit for extracting what we need in a human understandable form.\n",
    "    Beautiful soup parses anything we give it and does the tree traversal stuff for us.\n",
    "    \n",
    "    Why do we use it.\n",
    "    We make use of beautiful soup, in order to beautify extracted html code extracted from website so as to navigate through those links, elements, classes etc easily that are needed for our problem statement or project.\n",
    "    Beautiful soup can help us with locating content that is buried within the HTML structure, and helps us to select content based on tags.\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b0225e9-6358-4c80-ba5a-7840b871d8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why is flask used in this web scraping project?\n",
    "ans = \"\"\"\n",
    "    We have learned in previous lecture that flask is used to define our server or api's doing specific tasks.\n",
    "    Using flask we can render html pages too, as and when needed.\n",
    "    In our web-scrapping project, we first wrote a pythonic logic to extract data from website. Then our task were to,\n",
    "    display that extracted data in a clean way and that too on browser.\n",
    "    So we first created an html home-page where we will enter item to be searched for, and another template which display all the required data.\n",
    "    \n",
    "    Flask will help us in rendering templates dynamically and only when a specific api is called.\n",
    "    Apart from rendering templates, flask also helps in establishing link between our main logic and database collection where we can store our extracted data.\n",
    "    Using Flask we can abstract main logic and database connectivity, and can provide user a clean experience where he/she has to just enter a detail about the product and result will be displayed in a second.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59aafb1b-6626-4c96-bedf-5f47e9e65b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the names of AWS services used in this project. Also explain the use of each service?\n",
    "ans = \"\"\" \n",
    "    Aws services that were used in this project are codepipeline and Elastic Beanstalk.\n",
    "    Codepipeline:- Codepipeline act as a middleman between my github source code and Beanstalk.\n",
    "                   First I will need to push my project code to github account.\n",
    "                   Then I will create an pipeline via aws codepipeline linked to my project source code.\n",
    "                   Codepipeline provides a continuous delivery service required to release or deploy our software, so that \n",
    "                   any one can access our project.\n",
    "    BeanStalk:-   Now our code is linked, but we need enviournment to make my code perform and serve the community.\n",
    "                  Beanstalk provides us with this enviournment. It is a Platform As A Service as it allows users to directly,\n",
    "                  use a pre-configured server for their application. It abstracts the underlying configuration work and allows you as a user to focus on more pressing matters.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758f46c3-c568-4dc0-943e-ba5fac863735",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
