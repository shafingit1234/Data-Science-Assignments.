{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e07a0800-96b8-43a7-b76a-dd6901eeaee4",
   "metadata": {},
   "source": [
    "Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc86cdfb-97b0-469f-a0f0-398315868f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer = \"\"\" \n",
    "Missing data, or missing values, occur when you don’t have data stored for certain variables or participants.\n",
    "Data can go missing due to incomplete data entry, equipment malfunctions, lost files, and many other reasons.\n",
    "In any dataset, there are usually some missing data. In quantitative research, missing values appear as blank cells in your spreadsheet.\n",
    "\n",
    "Why is it Essential to handle missing values:-\n",
    "Training a model with a data set that has a lot of missing values can drastically impact the quality of machine learning model.\n",
    "Particularly missing values impact deterministic models.\n",
    "The fact is, majority of models that are most commonly in use today for practical applications are deterministic.\n",
    "Hence it becomes necessary to deal with missing data before applying the machine learning model.\n",
    "\n",
    "Algorithms that are not affected by missing values?\n",
    "K-Neared Neighbour Algorithm.\n",
    "Naive Bayes Algorithm.\n",
    "XGBoost Algorithm.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1ca4f7-6981-4501-bf5d-9dd2e5a56e25",
   "metadata": {},
   "source": [
    "Q2: List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6520c45a-91b9-44ef-af0f-64f357a894b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 11 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   survived  891 non-null    int64  \n",
      " 1   pclass    891 non-null    int64  \n",
      " 2   name      891 non-null    object \n",
      " 3   sex       891 non-null    object \n",
      " 4   age       714 non-null    float64\n",
      " 5   sibsp     891 non-null    int64  \n",
      " 6   parch     891 non-null    int64  \n",
      " 7   ticket    891 non-null    object \n",
      " 8   fare      891 non-null    float64\n",
      " 9   cabin     204 non-null    object \n",
      " 10  embarked  889 non-null    object \n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 76.7+ KB\n",
      "*********** Before handling missing data *********\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "survived      0\n",
       "pclass        0\n",
       "name          0\n",
       "sex           0\n",
       "age         177\n",
       "sibsp         0\n",
       "parch         0\n",
       "ticket        0\n",
       "fare          0\n",
       "cabin       687\n",
       "embarked      2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Answer = \"\"\"  \n",
    "1. Delete the Data.\n",
    "The easiest method is to just simply delete the whole training examples where one or several columns have null entries.\n",
    "Example:\n",
    "Let us import titanic dataset, and handle missing values over there.\n",
    "\"\"\"\n",
    "# Code handling missing data by deleting missing values.\n",
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import seaborn as sns;\n",
    "url = \"https://raw.github.com/mattdelhey/kaggle-titanic/master/Data/train.csv\";\n",
    "titanic = pd.read_csv(url);\n",
    "titanic.info();\n",
    "# count of null values before deletion.\n",
    "print(\"*********** Before handling missing data *********\");\n",
    "titanic.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c40aa48a-4ba3-4d4d-b89f-5749e01051b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* After handling missing data by removing nan values **************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "survived    0\n",
       "pclass      0\n",
       "name        0\n",
       "sex         0\n",
       "age         0\n",
       "sibsp       0\n",
       "parch       0\n",
       "ticket      0\n",
       "fare        0\n",
       "cabin       0\n",
       "embarked    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"************* After handling missing data by removing nan values **************\");\n",
    "titanic = titanic.dropna();\n",
    "titanic.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9a48f52-ea63-43db-863d-61974b45a3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************ data before imputing averages ************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Petranec, Miss. Matilda</td>\n",
       "      <td>female</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349245</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Petroff, Mr. Pastcho (\"Pentcho\")</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349215</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>White, Mr. Richard Frasar</td>\n",
       "      <td>male</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>35281</td>\n",
       "      <td>77.2875</td>\n",
       "      <td>D26</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johansson, Mr. Gustaf Joel</td>\n",
       "      <td>male</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7540</td>\n",
       "      <td>8.6542</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Gustafsson, Mr. Anders Vilhelm</td>\n",
       "      <td>male</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3101276</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Mionoff, Mr. Stoytcho</td>\n",
       "      <td>male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349207</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Salkjelsvik, Miss. Anna Kristine</td>\n",
       "      <td>female</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>343120</td>\n",
       "      <td>7.6500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Moss, Mr. Albert Johan</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>312991</td>\n",
       "      <td>7.7750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Rekic, Mr. Tido</td>\n",
       "      <td>male</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349249</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Miss. Bertha</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>371110</td>\n",
       "      <td>24.1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     survived  pclass                              name     sex   age  sibsp  \\\n",
       "100         0       3           Petranec, Miss. Matilda  female  28.0      0   \n",
       "101         0       3  Petroff, Mr. Pastcho (\"Pentcho\")    male   NaN      0   \n",
       "102         0       1         White, Mr. Richard Frasar    male  21.0      0   \n",
       "103         0       3        Johansson, Mr. Gustaf Joel    male  33.0      0   \n",
       "104         0       3    Gustafsson, Mr. Anders Vilhelm    male  37.0      2   \n",
       "105         0       3             Mionoff, Mr. Stoytcho    male  28.0      0   \n",
       "106         1       3  Salkjelsvik, Miss. Anna Kristine  female  21.0      0   \n",
       "107         1       3            Moss, Mr. Albert Johan    male   NaN      0   \n",
       "108         0       3                   Rekic, Mr. Tido    male  38.0      0   \n",
       "109         1       3               Moran, Miss. Bertha  female   NaN      1   \n",
       "\n",
       "     parch   ticket     fare cabin embarked  \n",
       "100      0   349245   7.8958   NaN        S  \n",
       "101      0   349215   7.8958   NaN        S  \n",
       "102      1    35281  77.2875   D26        S  \n",
       "103      0     7540   8.6542   NaN        S  \n",
       "104      0  3101276   7.9250   NaN        S  \n",
       "105      0   349207   7.8958   NaN        S  \n",
       "106      0   343120   7.6500   NaN        S  \n",
       "107      0   312991   7.7750   NaN        S  \n",
       "108      0   349249   7.8958   NaN        S  \n",
       "109      0   371110  24.1500   NaN        Q  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Answer = \"\"\"\n",
    "2. Imputing Averages.\n",
    "The next method is to assign some average value (mean, median or mode) to the null entries.\n",
    "\"\"\"\n",
    "# Example\n",
    "print(\"************ data before imputing averages ************\");\n",
    "# titanic[100 : 110]\n",
    "titanic = pd.read_csv(url);\n",
    "titanic[100 : 110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8130010c-563f-4011-b236-cb019d46123f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************** After imputing average values ********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_91/424377309.py:2: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  replace = titanic.mean();\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Millet, Mr. Francis Davis</td>\n",
       "      <td>male</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13509</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>E38</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Kenyon, Mrs. Frederick R (Marion)</td>\n",
       "      <td>female</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17464</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>D21</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Anderson, Mr. Harry</td>\n",
       "      <td>male</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19952</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>E12</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Gee, Mr. Arthur H</td>\n",
       "      <td>male</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111320</td>\n",
       "      <td>38.5000</td>\n",
       "      <td>E63</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Jerwan, Mrs. Amin S (Marie Marthe Thuillard)</td>\n",
       "      <td>female</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SC/AH Basle 541</td>\n",
       "      <td>13.7917</td>\n",
       "      <td>D</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Clifford, Mr. George Quincy</td>\n",
       "      <td>male</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>110465</td>\n",
       "      <td>52.0000</td>\n",
       "      <td>A14</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Bishop, Mr. Dickinson H</td>\n",
       "      <td>male</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11967</td>\n",
       "      <td>91.0792</td>\n",
       "      <td>B49</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Hoyt, Mrs. Frederick Maxfield (Jane Anne Forby)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19943</td>\n",
       "      <td>90.0000</td>\n",
       "      <td>C93</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Kent, Mr. Edward Austin</td>\n",
       "      <td>male</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11771</td>\n",
       "      <td>29.7000</td>\n",
       "      <td>B37</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Molson, Mr. Harry Markland</td>\n",
       "      <td>male</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113787</td>\n",
       "      <td>30.5000</td>\n",
       "      <td>C30</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     survived  pclass                                             name  \\\n",
       "456         0       1                        Millet, Mr. Francis Davis   \n",
       "457         1       1                Kenyon, Mrs. Frederick R (Marion)   \n",
       "460         1       1                              Anderson, Mr. Harry   \n",
       "462         0       1                                Gee, Mr. Arthur H   \n",
       "473         1       2     Jerwan, Mrs. Amin S (Marie Marthe Thuillard)   \n",
       "475         0       1                      Clifford, Mr. George Quincy   \n",
       "484         1       1                          Bishop, Mr. Dickinson H   \n",
       "486         1       1  Hoyt, Mrs. Frederick Maxfield (Jane Anne Forby)   \n",
       "487         0       1                          Kent, Mr. Edward Austin   \n",
       "492         0       1                       Molson, Mr. Harry Markland   \n",
       "\n",
       "        sex        age  sibsp  parch           ticket     fare cabin embarked  \n",
       "456    male  65.000000      0      0            13509  26.5500   E38        S  \n",
       "457  female  29.699118      1      0            17464  51.8625   D21        S  \n",
       "460    male  48.000000      0      0            19952  26.5500   E12        S  \n",
       "462    male  47.000000      0      0           111320  38.5000   E63        S  \n",
       "473  female  23.000000      0      0  SC/AH Basle 541  13.7917     D        C  \n",
       "475    male  29.699118      0      0           110465  52.0000   A14        S  \n",
       "484    male  25.000000      1      0            11967  91.0792   B49        C  \n",
       "486  female  35.000000      1      0            19943  90.0000   C93        S  \n",
       "487    male  58.000000      0      0            11771  29.7000   B37        C  \n",
       "492    male  55.000000      0      0           113787  30.5000   C30        S  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"************** After imputing average values ********\");\n",
    "replace = titanic.mean();\n",
    "# replace\n",
    "titanic = titanic.fillna(replace ,inplace = False);\n",
    "titanic[100 : 110]\n",
    "note = \"\"\" \n",
    "If you will carefully observe, the difference between two dataframes, you will notice that in latter case, nan are being replaced by the respective mean values.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7142db60-8662-4c84-b1a4-21beae7cdc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** Nan values replaced by unknown ***************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     survived  pclass                                               name  \\\n",
       "0           0       3                            Braund, Mr. Owen Harris   \n",
       "1           1       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
       "2           1       3                             Heikkinen, Miss. Laina   \n",
       "3           1       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
       "4           0       3                           Allen, Mr. William Henry   \n",
       "..        ...     ...                                                ...   \n",
       "886         0       2                              Montvila, Rev. Juozas   \n",
       "887         1       1                       Graham, Miss. Margaret Edith   \n",
       "888         0       3           Johnston, Miss. Catherine Helen \"Carrie\"   \n",
       "889         1       1                              Behr, Mr. Karl Howell   \n",
       "890         0       3                                Dooley, Mr. Patrick   \n",
       "\n",
       "        sex   age  sibsp  parch            ticket     fare    cabin embarked  \n",
       "0      male  22.0      1      0         A/5 21171   7.2500  Unknown        S  \n",
       "1    female  38.0      1      0          PC 17599  71.2833      C85        C  \n",
       "2    female  26.0      0      0  STON/O2. 3101282   7.9250  Unknown        S  \n",
       "3    female  35.0      1      0            113803  53.1000     C123        S  \n",
       "4      male  35.0      0      0            373450   8.0500  Unknown        S  \n",
       "..      ...   ...    ...    ...               ...      ...      ...      ...  \n",
       "886    male  27.0      0      0            211536  13.0000  Unknown        S  \n",
       "887  female  19.0      0      0            112053  30.0000      B42        S  \n",
       "888  female   NaN      1      2        W./C. 6607  23.4500  Unknown        S  \n",
       "889    male  26.0      0      0            111369  30.0000     C148        C  \n",
       "890    male  32.0      0      0            370376   7.7500  Unknown        Q  \n",
       "\n",
       "[891 rows x 11 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Answer = \"\"\" \n",
    "3. Assign New Category:\n",
    "In regards to the 'Cabin' feature, it only has 91 entries, which is about 25% of the total examples.\n",
    "Therefore, the mode value that we previously calculated is not very reliable. A better way is to assign these Nan Values their own category.\n",
    "\"\"\"\n",
    "titanic =pd.read_csv(url);\n",
    "titanic['cabin'] = titanic['cabin'].fillna('Unknown')\n",
    "print(\"*************** Nan values replaced by unknown ***************\");\n",
    "titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d056208-02d9-4a5c-9bce-6cdf20d1e2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer = \"\"\" \n",
    "4. Certain Algorithms:\n",
    "The final technique is to do nothing. The majority of machine learning algorithms do not work with missing data.\n",
    "On the other hand, algorithms as k-Nearest Neighbor, Naive Bayes, and XGBoost all work with missing data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828da140-94c3-446d-83ee-08d5d83140be",
   "metadata": {},
   "source": [
    "Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87ba1ec-7c1a-4148-bcba-4dc41a3997c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer = \"\"\"\n",
    "Imbalanced data refers to those types of datasets where the target class has an uneven distribution of observations,\n",
    "i.e one class label has a very high number of observations and the other has a very low number of observations. \n",
    "\n",
    "Assume we have a model having 1000 datapoints, on testing we find out that it is yielding results with 90% accuracy. For the sake of simplicity,\n",
    "let us assume we have either yes or no as our result.\n",
    "If a model is resulting yes 90% of time, it makes our model doubtful or kind of a dumb model if we have majority data points let's say 900 datapoints yeilding yes and 100 datapoints yeilding no.\n",
    "Such a model is said to have imbalanced data.\n",
    "If it is not handled, we are going to create a biased model, in this case a model that is going to say yes all the time.\n",
    "\n",
    "We generally follow two methods to handle imbalanced data.\n",
    "1. Upsampling.\n",
    "2. Downsampling.\n",
    "\n",
    "Explanation for the above two methods will be given in upcoming excercise problems.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a876cbca-6687-4a10-842d-cb201a6c5158",
   "metadata": {},
   "source": [
    "Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
    "sampling are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168902d1-7259-4653-9cfd-fdf960bdc88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer = \"\"\"\n",
    "Upsampling is a procedure where synthetically generated data points (corresponding to the minority class) are injected into the dataset.\n",
    "After this process, the counts of both labels are almost the same.\n",
    "This equalization procedure prevents the model from inclining towards the majority class.\n",
    "Furthermore, the interaction (Boundary line) between the target classes remains unaltered.\n",
    "\n",
    "Downsampling is a mechanism that reduces the count of training samples falling under the majority class.\n",
    "As it helps to even up the counts of target categories.\n",
    "By removing the collected data, we tend to lose so much valuable information.\n",
    "\n",
    "Example:-\n",
    "Take the model we discussed previously, where in we had 1000 datapoints with 900 datapoints corresponding to Yes, and remaining corresponding to No.\n",
    "To make this even, we can either add 900 datapoints yielding No into our dataset, that will make the count of datapoints equal to 2000.\n",
    "(upsampling mechanism).\n",
    "Or we can reduce the datapoints associated to a yes to a count of 100, making total datapoints as 200(downsampling).\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f06f5a-4165-470f-8b9e-1a321c9c0faf",
   "metadata": {},
   "source": [
    "Q5: What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f977763-b2ab-4fb1-9b00-341d7b7b100e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer = \"\"\" \n",
    "Data augmentation is a set of techniques to artificially increase the amount of data by generating new data points\n",
    "from existing data.\n",
    "This includes making small changes to data or using deep learning models to generate new data points.\n",
    "\n",
    "SMOTE:-\n",
    "(Synthetic Minority Oversampling Technique):\n",
    "SMOTE is an oversampling technique where the synthetic samples are generated for the minority class.\n",
    "This algorithm helps to overcome the overfitting problem posed by random oversampling.\n",
    "It focuses on the feature space to generate new instances with the help of interpolation between positive instances that lie together.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692570fa-8917-42d8-8f7a-1efdcc113224",
   "metadata": {},
   "source": [
    "Q6: What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2b3c05-f59b-4072-be6f-b165c0b6b611",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer = \"\"\" \n",
    "Anomalies of Outliers are those data points that lie at a great distance from the rest of the data like a sudden increase or decrease,\n",
    "by many folds or in the simple world an outlier is a value that lies outside the range of all other values in the dataset.\n",
    "For example, while measuring the body temperature of patients in a hospital there was an entry of 988 degrees Celsius which is clearly incorrect.\n",
    "There might be a missing decimal point like it should have been 98.8 instead of 988.\n",
    "\n",
    "Why is it essential to handle outliers:-\n",
    "If the outliers are not treated in the first step while doing the exploratory data analysis, it can lead to biases in the results.\n",
    "There are many unfavorable impacts created by a bias which could lead to poor business decisions and ultimately a loss to the business.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b31adb7-6371-4183-9426-142d632d3a4f",
   "metadata": {},
   "source": [
    "Q7: You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24881e44-2c4c-404a-b7e8-cb1a4a620345",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer = \"\"\" \n",
    "Techniques:-\n",
    "1. Imputing Averages:- Since we need to analyze customer data, deleting the data is not an option, so we can replace missing values with\n",
    "the mean value of that particular feature.\n",
    "2. Imputing Median, Mode\n",
    "3. Use certain algorithms like KNN, XGBoost.\n",
    "4. Delete the value if dealing with Missing at Random and Missing completely at Random data.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719cddf9-47f8-4272-bb48-9da6093bcbaf",
   "metadata": {},
   "source": [
    "Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73178cf-9c39-4b22-beaf-8979c61fcdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer = \"\"\"\n",
    "Missing at random data means that the reason for missing values can be explained by variables on which you have complete information,\n",
    "as there is some relationship between the missing data and other values/data. In such cases data is not missing for all ther observations.\n",
    "It is missing only within sub-samples of the data, and there is some pattern in the missing values.\n",
    "\n",
    "1. For such a case, it is relatively safer to delete data when compared to missing not at random data.\n",
    "We can perform deletion of rows or columns or pairwise deletion to get rid of missing value.\n",
    "1.1 Listwise deletion:-\n",
    "Listwise deletion (complete-case analysis) removes all data for an observation that has one or more missing values.\n",
    "Particularly if the missing data is limited to a small number of observations, you may just opt to eliminate those cases from the analysis.\n",
    "However in most cases, it is often disadvantageous to use listwise deletion.\n",
    "This is because the assumptions of MCAR (Missing Completely at Random) are typically rare to support.\n",
    "As a result, listwise deletion methods produce biased parameters and estimates.\n",
    "\n",
    "1.2 Pairwise Deletion:-\n",
    "pairwise deletion analyses all cases in which the variables of interest are present and thus maximizes all data available by an analysis basis.\n",
    "A strength to this technique is that it increases power in your analysis but it has many disadvantages. It assumes that the missing data are MCAR.\n",
    "If you delete pairwise then you’ll end up with different numbers of observations contributing to different parts of your model,\n",
    "which can make interpretation difficult.\n",
    "\n",
    "1.3 Dropping Variables:-\n",
    "In my opinion, it is always better to keep data than to discard it.\n",
    "Sometimes you can drop variables if the data is missing for more than 60% observations but only if that variable is insignificant.\n",
    "Having said that, imputation is always a preferred choice over dropping variables\n",
    "\n",
    "2. Imputing Mean, Median or Mode:-\n",
    "Computing the overall mean, median or mode is a very basic imputation method,\n",
    "it is the only tested function that takes no advantage of the time series characteristics or relationship between the variables.\n",
    "It is very fast, but has clear disadvantages. One disadvantage is that mean imputation reduces variance in the dataset.\n",
    "\n",
    "3. Use algorithms like KNN or XGBoost that works fine with missing values as well.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e561fc1f-63df-49ca-9a1d-b0b4d25e13dd",
   "metadata": {},
   "source": [
    "Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdd1798-07e5-4210-8bad-490e2e2274f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer = \"\"\"\n",
    "Approaches that we can follow:-\n",
    "1. Choose Proper Evaluation Metric:-\n",
    "The accuracy of a classifier is the total number of correct predictions by the classifier divided by the total number of predictions. This may be good enough for a well-balanced class but not ideal for the imbalanced class problem. The other metrics such as precision is the measure of how accurate the classifier’s prediction of a specific class and recall is the measure of the classifier’s ability to identify a class.\n",
    "\n",
    "For an imbalanced class dataset F1 score is a more appropriate metric. It is the harmonic mean of precision and recall\n",
    "So, if the classifier predicts the minority class but the prediction is erroneous and false-positive increases,\n",
    "the precision metric will be low and so as F1 score. Also, if the classifier identifies the minority class poorly,\n",
    "i.e. more of this class wrongfully predicted as the majority class then false negatives will increase, so recall and F1 score will low.\n",
    "F1 score only increases if both the number and quality of prediction improves.\n",
    "2. Resampling:-\n",
    "We can make use of upsampling and downsampling techniques.\n",
    "\n",
    "3. SMOTE:- Synthetic Minority Oversampling Technique or SMOTE is another technique to oversample the minority class.\n",
    "In Smote new instances are synthesized from the existing data.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a71b45-67b7-4a29-9d4d-2c620fdf3a7f",
   "metadata": {},
   "source": [
    "Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdb5e01-3921-404b-a7e8-3c0ae24dda1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer = \"\"\" \n",
    "1. Random undersampling: This method involves randomly selecting a subset of observations from the majority class to match the size of the minority class.\n",
    "This can be done using techniques such as RandomUnderSampler from the imblearn library in Python.\n",
    "\n",
    "2. Tomek links: This method involves identifying pairs of observations that are nearest neighbors and belong to different classes.\n",
    "The observation from the majority class is then removed to balance the dataset.\n",
    "This can be done using techniques such as Tomek Links from the imblearn library in Python.\n",
    "\n",
    "3. Synthetic minority oversampling technique (SMOTE): This method involves generating synthetic observations for the minority class,\n",
    "to match the size of the majority class. SMOTE can be used in combination with random undersampling to balance the dataset.\n",
    "This can be done using techniques such as SMOTE Tomek from the imblearn library in Python.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e751638c-e155-464c-973c-491fcb9a4c84",
   "metadata": {},
   "source": [
    "Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0594534-06e1-47c8-84ff-7c8f105dc963",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer = \"\"\" \n",
    "1. Random oversampling: This method involves randomly duplicating observations from the minority class to match the size of the majority class.\n",
    "This can be done using techniques such as RandomOverSampler from the imblearn library in Python.\n",
    "\n",
    "2. Synthetic minority oversampling technique (SMOTE): This method involves generating synthetic observations for the minority class,\n",
    "to match the size of the majority class. SMOTE can be used in combination with random oversampling to balance the dataset.\n",
    "This can be done using techniques such as SMOTE from the imblearn library in Python.\n",
    "\n",
    "3. Adaptive synthetic sampling (ADASYN): This method is similar to SMOTE but focuses on generating more synthetic observations for the minority class,\n",
    "samples that are harder to learn. This can be done using techniques such as ADASYN from the imblearn library in Python.\n",
    "\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
